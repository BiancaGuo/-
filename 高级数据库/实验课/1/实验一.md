## 大数据案例-步骤一:本地数据集上传到数据仓库Hive

### 一、任务清单

* 安装Linux系统
* 数据集下载与查看
* 数据集预处理
* 把数据集导入分布式文件系统HDFS中
* 在数据仓库Hive上创建数据库

### 二、实验结果

* 安装Linux系统

	* 本实验采用ubuntu 16.04 server版
	* IP:192.169.227.10

	![](https://i.imgur.com/Px9X1kE.jpg)

* 数据集下载与查看

	* 查看raw_user.csv前五条数据：`head -5 raw_user.csv`

	![](https://i.imgur.com/Gm2Mrat.jpg)

* 数据集预处理

	* 删除文件第一行记录，即字段名称

	![](https://i.imgur.com/d5DBU6z.jpg)

* 对字段进行预处理

	* 为每行记录增加一个id字段（让记录具有唯一性）、增加一个省份字段（用来后续进行可视化分析），并且丢弃user_geohash字段（后面分析不需要这个字段）。

	![](https://i.imgur.com/JW01GYt.jpg)

* 导入数据库

	* 启动hadoop：`./sbin/start-all.sh`

	![](https://i.imgur.com/qF7qXr4.jpg)

	* 把user_table.txt上传到HDFS中，查看一下HDFS中的user_table.txt的前10条记录

	![](https://i.imgur.com/yV99WOj.jpg)

	* 在Hive上创建数据库，创建外部表，查询数据

		* select * from bigdata_user limit 10;

		![](https://i.imgur.com/GakAGsF.jpg)

		* select behavior_type from bigdata_user limit 10;

		![](https://i.imgur.com/17z12LA.jpg)